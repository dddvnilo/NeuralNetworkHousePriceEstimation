COMPLETE EXPERIMENT RANKING
====================================================

METRICS EXPLANATION:
- Epochs: Maximum epochs allowed (early stop)
- PCA: Number of PCA components (0 = no PCA)
- LR: Learning Rate
- Layers: Hidden Layer architecture
- Dropout: Dropout rate
- RMSE: Root Mean Squared Error (lower = better)
- MAE: Mean Absolute Error (lower = better)
- Actual Epochs: Epochs used before early stopping

EXPERIMENT RANKING:
===================

1. **FIRST**
   Command: python regresion_nn.py --epochs 500 --pca_components 30 --lr 0.01 --hidden_layers 256 128 64 --dropout 0.0
   RMSE: 30140, MAE: 18765, R²: 0.8818, Actual Epochs: 29

2. **SECOND**
   Command: python regresion_nn.py --epochs 500 --pca_components 25 --lr 0.01 --hidden_layers 128 64 --dropout 0.1
   RMSE: 30516, MAE: 18725, R²: 0.8788, Actual Epochs: 60

3. **THIRD**
   Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.01 --hidden_layers 128 64 --dropout 0.0
   RMSE: 30990, MAE: 18822, R²: 0.8750, Actual Epochs: 91

4. **FOURTH**
   Command: python regresion_nn.py --epochs 500 --pca_components 35 --lr 0.001 --hidden_layers 256 128 64 --dropout 0.1
   RMSE: 30912, MAE: 19094, R²: 0.8756, Actual Epochs: 59

5. **FIFTH**
   Command: python regresion_nn.py --epochs 500 --pca_components 35 --lr 0.001 --hidden_layers 256 128 64 --dropout 0.2
   RMSE: 31012, MAE: 18931, R²: 0.8748, Actual Epochs: 94

6. **SIXTH**
   Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.01 --hidden_layers 256 128 64 --dropout 0.15
   RMSE: 31577, MAE: 20170, R²: 0.8702, Actual Epochs: 33

7. **SEVENTH**
   Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.005 --hidden_layers 128 64 --dropout 0.0
   RMSE: 31709, MAE: 19687, R²: 0.8691, Actual Epochs: 95

8. **EIGHTH**
   Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.01 --hidden_layers 128 64 --dropout 0.2
   RMSE: 31675, MAE: 20065, R²: 0.8694, Actual Epochs: 58

9. **NINTH**
   Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.01 --hidden_layers 256 128 64 --dropout 0.1
   RMSE: 31737, MAE: 19508, R²: 0.8689, Actual Epochs: 28

10. **TENTH**
    Command: python regresion_nn.py --epochs 500 --pca_components 35 --lr 0.001 --hidden_layers 256 128 64 --dropout 0.0
    RMSE: 31823, MAE: 19830, R²: 0.8682, Actual Epochs: 37

ADDITIONAL EXPERIMENTS:
=======================

11. Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.01 --hidden_layers 256 128 64 --dropout 0.0
    RMSE: 32273, MAE: 21148, Actual Epochs: 39

12. Command: python regresion_nn.py --epochs 300 --pca_components 20 --lr 0.01 --hidden_layers 128 64 --dropout 0.0
    RMSE: 31831, MAE: 19896, Actual Epochs: 300

13. Command: python regresion_nn.py --epochs 500 --pca_components 25 --lr 0.001 --hidden_layers 256 128 64 --dropout 0.0
    RMSE: 35582, MAE: 23850, Actual Epochs: 500

14. Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.001 --hidden_layers 256 128 64 --dropout 0.1
    RMSE: 32727, MAE: 20564, Actual Epochs: 299

15. Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.001 --hidden_layers 256 128 64 --dropout 0.2
    RMSE: 33096, MAE: 20869, Actual Epochs: 299

16. Command: python regresion_nn.py --epochs 500 --pca_components 20 --lr 0.0001 --hidden_layers 128 64 --dropout 0.0
    RMSE: 61813, MAE: 49203, Actual Epochs: 500

KEY FINDINGS:
=============
OPTIMAL CONFIGURATION:
PCA: 30 components
Learning Rate: 0.01
Architecture: [256, 128, 64]
Dropout: 0.0 (none)
Training: 29 epochs (early stopping)

PCA ANALYSIS:
Best: PCA 30 (R²: 0.8818)
Very Good: PCA 25 (R²: 0.8788)
Acceptable: PCA 20, 35 (R²: 0.8750-0.8756)
Poor: PCA <20 or >35

LEARNING RATE ANALYSIS:
Best: 0.01 (all top 3 configurations)
Acceptable: 0.005, 0.001
0.01 consistently outperforms lower learning rates

ARCHITECTURE ANALYSIS:
Best: [256, 128, 64] with PCA 30
Very Good: [128, 64] with PCA 25
Complex architectures benefit from higher PCA components
Simpler architectures more stable across different PCA values

DROPOUT ANALYSIS:
Dropout consistently provided no benefit or worsened performance
Top 3 configurations all use Dropout 0.0
Model shows no signs of overfitting with optimal parameters
Regularization unnecessary for this dataset/config

TRAINING EFFICIENCY:
Best model trains in only 29 epochs
Typical training range: 29-95 epochs
Lower learning rates (0.001) require more training time
Optimal configurations converge quickly

PERFORMANCE RANGES:
===================
Best RMSE: 30,140 (R²: 0.8818)
Worst RMSE: 33,413 (R²: 0.8547)
Typical RMSE: 30,000 - 32,000
Best R²: 0.8818
Worst R²: 0.8547
Typical R²: 0.868 - 0.882